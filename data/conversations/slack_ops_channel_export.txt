# TechCorp #infrastructure-ops Slack Channel Export
# Export Date: 2024-10-05
# Date Range: 2024-09-12 to 2024-10-05
# Channel Members: 42

==================================================
Thread: Payment Gateway Incident - Sep 12
Started: 2024-09-12 14:20:11 by dev.patel
==================================================

[14:20:11] dev.patel: ðŸš¨ SEV-1 alert - payment gateway completely down, latency through the roof. Creating #incident-2024-09-12-payment-timeout

[14:23:42] lisa.wong: @dev.patel I'm looking at DB metrics, everything looks normal on our end. Connection count is stable

[14:26:18] dev.patel: That's weird, we're seeing timeout errors. Let me check connection pool utilization

[14:31:55] tom.chen: Do you need infrastructure help? I can jump in

[14:33:21] dev.patel: Not sure yet, still diagnosing. Service is timing out but can't find why

[14:38:47] sarah.chen: Escalating to SEV-1, this is affecting all customers. @dev.patel what do you need?

[14:41:03] dev.patel: Honestly not sure. This is outside my expertise. Database-related but platform team says DB is fine

[14:52:19] dev.patel: Rolling back last week's deployment just in case

[14:58:33] dev.patel: Rollback complete - NO EFFECT. Still down ðŸ˜°

[15:16:44] marcus.rodriguez: @lisa.wong can you take a deeper look at database connections? Something must be wrong there

[15:21:07] lisa.wong: Found it - connection pool is at 100% utilization! But I don't know why connections aren't being released

[15:42:18] tom.chen: Could this be network-related? We had some weird AWS networking stuff last month

[15:48:22] tom.chen: Checked network metrics - all nominal. Not network.

[15:51:09] jake.chen: Hey I remember billing team had something similar a while back. @anna.kim do you remember what that was?

[15:56:34] anna.kim: Oh yeah! We had connection leaks from unclosed database cursors. Took forever to debug. Let me find the postmortem...

[16:02:11] anna.kim: Can't find the postmortem in Confluence ðŸ˜ž I think it was INC-2024-021? But I remember the issue - async handlers weren't properly releasing connections in error paths

[16:09:47] dev.patel: OMG that might be it. Let me check our async query handlers

[16:16:23] dev.patel: FOUND IT! There's a code path that returns without releasing the connection. Writing hotfix now

[16:43:51] dev.patel: Hotfix deployed. Connection pool is draining ðŸ™

[17:05:33] sarah.chen: Great work everyone. Dev, make sure we do a thorough postmortem on this one. Took way too long to resolve.

[17:12:44] marcus.rodriguez: Agreed. We need to figure out why it took 2 hours to find Anna who had already solved this problem

==================================================
Thread: Do we need a centralized SRE team?
Started: 2024-09-18 10:15:33 by sarah.chen
==================================================

[10:15:33] sarah.chen: Based on last week's incident and the postmortem discussion, I want to get everyone's thoughts on creating a dedicated SRE team to handle incidents. Particularly interested in hearing from folks currently on-call.

[10:22:47] lisa.wong: I'm strongly in favor. Platform team is supposed to build infrastructure features but we spend 40-50% of our time on incidents and cross-team support. A dedicated SRE team could handle incident coordination while we focus on our roadmap.

[10:24:19] dev.patel: +1 from me. Payment team is drowning. We get 3x more incidents than anyone else and our on-call rotation is every 10 days. It's unsustainable. I'd love to hand off incident response to specialists.

[10:31:52] tom.chen: I'm skeptical. I've worked at companies with SRE teams before and it creates a "throw it over the wall" mentality. Product teams stop caring about reliability because "SRE will handle it." We lose ownership culture.

[10:34:08] anna.kim: I see both sides. Yes, we need better knowledge sharing (case in point: last week's incident). But I also worry about losing context. When I'm on-call for billing, I KNOW the system. An SRE might not.

[10:39:41] jake.chen: What if we did a hybrid? Keep team ownership but have dedicated incident commanders who coordinate? They don't fix the problem, they just help organize the response.

[10:42:15] lisa.wong: @jake.chen That's interesting. So product teams still own and respond, but IC role is dedicated? Could work.

[10:45:29] tom.chen: I could get behind that. My main concern is ownership. As long as my team still owns our services end-to-end, having help with coordination would be great.

[10:51:03] rachel.kumar: From product side - I'm worried about velocity. If we create an SRE team, do incidents become "their problem"? Do we have to wait in queue for them to help us? I don't want a bottleneck.

[10:56:47] dev.patel: @rachel.kumar I get that concern but honestly I'm so burned out that I'd take a small delay if it means I'm not woken up 3x per week

[11:03:22] marcus.rodriguez: This is really valuable feedback. I think we need to look at options:
1. Full SRE team (centralized incident response)
2. Status quo but with improvements (IC role, better tools)
3. Hybrid model (dedicated coordination, teams still own)

[11:08:15] sarah.chen: Agreed. I'll put together a proposal with options and we'll review in Q4 planning. Keep the feedback coming.

[11:15:42] emma.davis: From QA perspective - whatever we choose, please consolidate the monitoring tools. I have to check 4 different dashboards to understand system health. It's ridiculous.

[11:18:29] lisa.wong: @emma.davis YES. Tool consolidation should happen regardless of org structure decision. Total waste of time and money.

==================================================
Thread: Payment Team Burnout
Started: 2024-09-25 15:42:18 by marcus.rodriguez
==================================================

[15:42:18] marcus.rodriguez: Checking in with @dev.patel and payment team. I know you've had a rough quarter with incidents. How's everyone doing?

[15:48:53] dev.patel: Honestly? Not great. Between you and me, I'm updating my resume. Love the company but the on-call burden is killing me. Every 10 days I'm back on-call and we average 5-6 incidents per week.

[15:51:34] marcus.rodriguez: That's concerning. What would help in the short term while we figure out longer-term solutions?

[15:55:07] dev.patel: 1) Hire someone for payment team - we're at 3 people, should be 5
2) Better runbooks so I'm not reinventing the wheel every incident
3) Knowledge from other teams - we keep hitting problems others have solved
4) Maybe a break from on-call? I've been in rotation for 8 months straight

[16:02:41] sarah.chen: @dev.patel I'm reading this. Let's talk 1:1 tomorrow. In the meantime, you're off on-call rotation for the next month. We'll figure out coverage.

[16:05:19] dev.patel: Thank you ðŸ™

[16:12:45] anna.kim: @dev.patel solidarity. Billing team isn't as bad as payment but we feel it too. The incident load is just growing and we're not growing headcount proportionally.

[16:18:33] lisa.wong: This is why I think we need structural changes. Individual team scaling isn't the answer - we need centralized expertise and better incident response processes.

[16:24:07] tom.chen: Or we need to fix the root causes. Why are we having so many incidents? Are we moving too fast? Skipping testing? Maybe we should slow down feature velocity and focus on stability.

[16:31:52] jake.chen: @tom.chen I don't think velocity is the issue. Product-backend team ships twice as much and has 10x fewer incidents. I think it's about which services are in the critical path and how much tech debt they have.

[16:35:41] dev.patel: Payment service is definitely tech debt city. We've been "planning to refactor" for 18 months but never have time because we're always firefighting.

[16:42:19] sarah.chen: Vicious cycle. Incidents â†’ no time to fix â†’ more incidents. We need to break that cycle. Q4 planning will address this.

==================================================
Thread: Tool Consolidation Discussion
Started: 2024-09-28 11:25:07 by emma.davis
==================================================

[11:25:07] emma.davis: Following up on the tool consolidation comment from the other thread. What are we using right now?

[11:28:44] lisa.wong: Deep breath... DataDog (APM), New Relic (legacy monitoring), Prometheus + Grafana (infra metrics), Sentry (errors), PingDom (uptime checks)

[11:30:12] tom.chen: Don't forget PagerDuty for alerting

[11:31:55] emma.davis: FIVE monitoring tools?? Why??

[11:34:38] lisa.wong: Historical reasons. New Relic was first, then we added DataDog for better APM. DevOps team prefers Prometheus. Sentry is best-in-class for error tracking. PingDom is cheap for external monitoring.

[11:39:21] tom.chen: Each tool made sense at the time but now we have fragmentation. Different teams look at different tools. During incidents I'm toggling between 3 dashboards.

[11:43:09] jake.chen: What's the cost for all this?

[11:45:52] lisa.wong: ~$178K/year total. DataDog is $95K, New Relic $42K, Sentry $28K, PingDom $13K. Prometheus is self-hosted so just server costs.

[11:49:37] marcus.rodriguez: Could we consolidate to just DataDog? I know it can do most of what the others do.

[11:53:18] lisa.wong: Yes! DataDog can replace New Relic, Prometheus, and PingDom. We'd keep Sentry because it's purpose-built for error tracking. Would save ~$60K/year and massive time savings.

[11:56:43] tom.chen: I'm resistant to giving up Prometheus. It's more flexible and we have custom dashboards. DataDog can be a black box.

[12:01:29] lisa.wong: @tom.chen I get it, but is the flexibility worth the operational overhead? Honest question. How often do you use Prometheus features that DataDog doesn't have?

[12:05:47] tom.chen: Fair point. Maybe monthly? And usually for deep debugging. Most day-to-day stuff DataDog could handle.

[12:10:15] jake.chen: Can we do a trial? Migrate one service to DataDog fully and see if we miss anything from the other tools?

[12:14:38] lisa.wong: Good idea. I can migrate notification-service next week. It's currently on all 3 systems (New Relic, DataDog, Prometheus) so we can do apples-to-apples comparison.

[12:18:52] sarah.chen: Love this initiative. @lisa.wong go ahead with the trial. If it works, let's plan full migration for Q4.

[12:23:07] emma.davis: Please yes. Having a single source of truth would be so much better for QA testing too.

==================================================
Thread: Incident Commander Role Proposal
Started: 2024-10-02 09:15:44 by marcus.rodriguez
==================================================

[09:15:44] marcus.rodriguez: Want to float an idea: What if we create a rotating "Incident Commander" role for SEV-1/SEV-2 incidents? The IC wouldn't fix the problem, they'd coordinate the response.

[09:21:33] lisa.wong: Like an air traffic controller for incidents? I like it. What would they do specifically?

[09:24:18] marcus.rodriguez: IC would:
- Join incident immediately
- Make sure right people are looped in
- Coordinate communication (status updates, customer comms)
- Make decisions when teams disagree
- Keep timeline/notes
- Drive postmortem process

Engineers still do the technical work, IC handles coordination.

[09:31:47] anna.kim: This would have helped SO MUCH during the payment incident last month. So much time wasted on "who should we call?" "has anyone updated executives?" "wait, what did we try already?"

[09:35:02] dev.patel: +1. I was trying to fix the problem AND coordinate 7 people AND update Slack AND talk to customer support. Too many hats.

[09:39:28] tom.chen: Who would be IC? Would it be a full-time role or rotation?

[09:42:51] marcus.rodriguez: I'm thinking rotation among senior engineers. Like on-call but specifically for incident coordination. Maybe 1 week rotations, pool of 8-10 people?

[09:47:13] jake.chen: Would there be additional compensation? It's extra responsibility.

[09:50:37] marcus.rodriguez: Yes, probably similar to on-call pay. Maybe $500/week when you're the IC on rotation.

[09:54:22] rachel.kumar: I'd volunteer for this. I actually enjoy the coordination aspect more than the deep debugging. Playing to people's strengths.

[09:58:45] sarah.chen: This feels like a quick win we could implement now while we debate bigger org changes. @marcus.rodriguez can you draft a proposal?

[10:03:17] marcus.rodriguez: On it. Will have something by end of week.

[10:07:55] lisa.wong: One concern - does IC need to be technical enough to understand what's happening? Or purely coordination?

[10:12:31] marcus.rodriguez: Good question. I think they need enough technical background to ask good questions and understand trade-offs, but they don't need to be experts in that specific service.

[10:16:48] anna.kim: Senior engineer level minimum then. Someone with 3+ years at the company who's seen multiple services.

[10:20:19] marcus.rodriguez: Agreed. Will include that in the proposal.

==================================================
Thread: Follow-up on Q4 Planning
Started: 2024-10-05 14:30:22 by sarah.chen
==================================================

[14:30:22] sarah.chen: Quick update on Q4 planning doc I'm putting together. Based on all the discussions here, I'm proposing three options:

Option A: Full centralized SRE team (6 people)
Option B: Enhanced status quo (hire 2 platform engineers, add IC role, better processes)
Option C: Hybrid model (3-person incident response team for coordination + keep team ownership)

Draft will be ready next week. Thoughts on these directions?

[14:36:49] lisa.wong: I'm most excited about Option A but Option C seems like a good middle ground if there's budget/hiring concerns.

[14:39:14] tom.chen: Option B for me. I still think we can solve this with better processes and a couple more people. Don't need full restructuring.

[14:43:27] dev.patel: Honestly anything is better than current state. Option A would give me most relief but I'd take Option C too.

[14:47:52] anna.kim: What's the cost difference between options?

[14:51:38] sarah.chen: Rough numbers:
A: ~$750K net increase (6 SRE salaries - some savings)
B: ~$270K (2 engineers + IC stipend)
C: ~$450K (3 IC engineers + 1 platform engineer)

Budget approved is $450K so Option C fits perfectly, Option A would need additional approval.

[14:56:13] jake.chen: Option C sounds right-sized then. Addresses the coordination problem (which is real) without over-rotating.

[15:01:47] rachel.kumar: Does Option C have a path to grow into Option A later if needed? Or is it a different direction entirely?

[15:05:21] sarah.chen: @rachel.kumar Good question. Yes, Option C could evolve into Option A. The 3-person IR team could expand to 6-person SRE team if incident volume keeps growing. It's more incremental.

[15:09:45] tom.chen: That actually makes me feel better about Option C. Lower risk, can adjust if needed.

[15:14:16] lisa.wong: Okay I'm convinced. Option C > Option A for pragmatic reasons. Still gets us incident coordination and knowledge consolidation.

[15:18:53] marcus.rodriguez: Sounds like rough consensus forming around Option C? Plus tool consolidation regardless of which option?

[15:22:37] sarah.chen: That's what I'm hearing too. I'll flesh out Option C in detail in the planning doc. Tool consolidation is happening regardless - @lisa.wong already proved it works with the notification-service trial.

[15:27:04] emma.davis: Thank you for including everyone in this discussion. Feels collaborative.

[15:30:41] sarah.chen: Of course. These decisions affect everyone. Will share full draft next week for detailed feedback before leadership review.

==================================================
Thread: Monitoring Trial Results
Started: 2024-10-04 16:45:12 by lisa.wong
==================================================

[16:45:12] lisa.wong: Update on the DataDog consolidation trial. Migrated notification-service fully to DataDog, turned off New Relic and Prometheus for that service.

Results after 1 week: âœ… Everything we need is in DataDog

[16:49:33] tom.chen: What about the custom Prometheus dashboards we had?

[16:52:18] lisa.wong: Rebuilt them in DataDog. Took about 3 hours but now they're actually better because they correlate with APM traces and logs in the same view.

[16:56:47] tom.chen: Okay I'm sold. Let's do the full migration.

[17:01:24] lisa.wong: I'll create a migration plan. Thinking we do it in phases:
Phase 1: Migrate New Relic services (November)
Phase 2: Migrate Prometheus (December)
Phase 3: Decommission old tools (January)

[17:05:39] marcus.rodriguez: What's the risk if DataDog goes down?

[17:08:51] lisa.wong: Good question. We'd keep PagerDuty for alerting (it has redundant health checks). And we can keep Prometheus in read-only mode for 1 month during transition as backup.

[17:13:16] sarah.chen: Sounds well thought out. Approved to proceed. Add it to Q4 planning.

[17:17:42] emma.davis: One practical question - will this change how we write application metrics? Do we need to update instrumentation?

[17:21:29] lisa.wong: DataDog agent is mostly compatible with Prometheus format so minimal code changes. Main work is dashboard/alert migration, not instrumentation.

[17:25:51] jake.chen: Timeline-wise this fits nicely. If we're also doing org changes (IC team), having consolidated tooling first will make their lives easier.

[17:30:18] lisa.wong: Exactly. Single pane of glass for IR team to work from.

==================================================

# End of export
# Total messages: 78
# Total threads: 6
# Active participants: 12
# Date range: 2024-09-12 to 2024-10-05
